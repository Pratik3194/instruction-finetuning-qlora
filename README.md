# Instruction Finetuning with QLoRA

This repository contains an implementation of **instruction finetuning** using **QLoRA (Quantized Low-Rank Adaptation)** for **CodeLlama**.  
The goal is to perform **Supervised Finetuning (SFT)** on function-calling tasks using **PEFT (Parameter-Efficient Fine-Tuning)** methods.

---

## ğŸš€ Features

- Fine-tuning CodeLlama with QLoRA for instruction-following tasks  
- Parameter-efficient approach using PEFT  
- Demonstrates function-calling use cases  
- Google Colab-compatible implementation  

---

## ğŸ“‚ Project Structure

- `instruction-finetuning-qlora-assignment.ipynb` : Main notebook with the complete workflow  

---

## âš™ï¸ Installation

Clone the repository and install the requirements:

```bash
git clone https://github.com/Pratik3194/instruction-finetuning-qlora.git
cd instruction-finetuning-qlora
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

Run the notebook step by step:

1. Mount Google Drive (if using Colab)
2. Install dependencies
3. Login to Weights & Biases (W&B) for experiment tracking
4. Load dataset and preprocess
5. Fine-tune CodeLlama using QLoRA + PEFT
6. Evaluate and test the model

---

## ğŸ“Š Results

- Successfully fine-tuned CodeLlama for function-calling tasks
- Demonstrates efficient training with reduced GPU memory usage

---

## ğŸ“Œ Requirements

- Python 3.9+
- PyTorch
- Hugging Face Transformers
- PEFT
- bitsandbytes
- accelerate
- wandb
